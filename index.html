<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>John  See</title>
    <meta name="author" content="John  See">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/faviconV2.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://john-see.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="mailto:%6A.%73%65%65@%68%77.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-3005-4109" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=uH74dcgAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.semanticscholar.org/author/143937986" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a>
            <a href="https://publons.com/a/C-8633-2013/" title="Publons" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons"></i></a>
            <a href="https://www.scopus.com/authid/detail.uri?authorId=56227831000" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a>
            <a href="https://github.com/jstulips" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/john-see" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/johnseesuyang" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            <a href="https://dblp.org/pid/36/4809.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/mudalab/">mudalab</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            <span style="font-weight: 500">John</span>
              
              See  <br class="d-md-none">(施
                <span style="font-weight: 500">书杨</span>)
          </h1>
          <p class="desc"><small>Ph.D, SMIEEE</small>.</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/john-pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/john-pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/john-pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/john-pic.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="john-pic.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>Hi! I am an Associate Professor at the <a href="https://www.hw.ac.uk/uk/schools/mathematical-computer-sciences/about.htm" rel="external nofollow noopener" target="_blank">School of Mathematical and Computer Sciences</a>, <a href="https://www.hw.ac.uk/malaysia/" rel="external nofollow noopener" target="_blank">Heriot-Watt University (Malaysia Campus)</a>. I spent much of my early career at <a href="http://www.mmu.edu.my/" rel="external nofollow noopener" target="_blank">Multimedia University</a>, Malaysia where I founded the <a href="https://viprlab.github.io/" rel="external nofollow noopener" target="_blank">Visual Processing (ViPr) Lab</a> and was also chair of the Centre for Visual Computing (CVC). From 2017-2019, I was also a Visiting Research Fellow at <a href="http://en.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Shanghai Jiao Tong University (SJTU)</a> as a recepient of the Belt and Road Initiative Young Scientist Fellowship where I worked closely with <a href="https://weiyaolin.github.io/" rel="external nofollow noopener" target="_blank">Weiyao Lin</a>.</p>

<p>My research interests spans broadly the areas of computer vision and multimedia signal processing. In particular, my current research focus is on the understanding and analysis of emotional signals from a variety of context - from photographs to subtle facial cues to human-robot interactions. I still work on a lot of classical problems related to visual surveillance: object detection from high resolution imagery, activity recognition and localization.</p>

<p>I am currently serving as the Subject Editor (Senior Area Chair) of the <a href="https://www.sciencedirect.com/journal/signal-processing" rel="external nofollow noopener" target="_blank">Signal Processing</a> journal, and Associate Editor of <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia" rel="external nofollow noopener" target="_blank">IEEE Transactions on Multimedia</a>, <a href="https://jivp-eurasipjournals.springeropen.com/" rel="external nofollow noopener" target="_blank">EURASIP Journal of Image and Video Processing</a>, <a href="https://academic.oup.com/comjnl/" rel="external nofollow noopener" target="_blank">The Computer Journal</a> and <a href="https://www.frontiersin.org/journals/signal-processing" rel="external nofollow noopener" target="_blank">Frontiers in Signal Processing - Image Processing</a>. I am also a Senior Member of IEEE and an elected member of the IEEE Multimedia Systems and Applications (MSA) Technical Committee (2020-2024).</p>

<!--
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.test

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.
-->

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Jan 30, 2024</th>
                  <td>
                    New Year, new design. My new site is now relaunched in a fresher, minimalist format!

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="sfamnetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/sfamnet.png"><div id="sfamnetpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('sfamnetpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="sfamnetpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("sfamnetpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('sfamnetpng');
      var modalImg = document.getElementById("sfamnetpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="liong2024sfamnet" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">SFAMNet: A Scene Flow Attention-based Micro-expression Network</div>
    <!-- Author -->
    <div class="author">
      

      Gen-Bing Liong, Sze-Teng Liong, Chee Seng Chan, and <em>John See</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Neurocomputing</em>, 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/10015091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://www.techrxiv.org/articles/preprint/NeSVoR_Implicit_Neural_Representation_for_Slice-to-Volume_Reconstruction_in_MRI/21398868" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/genbing99/SFAMNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.neucom.2023.126998"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1016/j.neucom.2023.126998" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.neucom.2023.126998" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Tremendous progress has been made in facial Micro-Expression (ME) spotting and recognition; however, most works have focused on either spotting or recognition tasks on the 2D videos. Until recently, the estimation of the 3D motion field (a.k.a scene flow) for the ME has only become possible after the release of the multi-modal ME dataset. In this paper, we propose the first Scene Flow Attention-based Micro-expression Network, namely SFAMNet. It takes the scene flow computed using the RGB-D flow algorithm as the input and predicts the spotting confidence score and emotion labels. Specifically, SFAMNet is an attention-based end-to-end multi-stream multi-task network devised to spot and recognize the ME. Besides that, we present a data augmentation strategy to alleviate the small sample size problem during network learning. Extensive experiments are performed on three tasks: (i) ME spotting; (ii) ME recognition; and (iii) ME analysis on the multi-modal CAS(ME)^3 dataset. Empirical results indicate that depth is vital in capturing the ME information and the effectiveness of the proposed approach. Our source code is publicly available at <a href="https://github.com/genbing99/SFAMNet" rel="external nofollow noopener" target="_blank">https://github.com/genbing99/SFAMNet</a>.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="moiredetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/moiredet.png"><div id="moiredetpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('moiredetpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="moiredetpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("moiredetpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('moiredetpng');
      var modalImg = document.getElementById("moiredetpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="yang2023doing" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Doing More With Moiré Pattern Detection in Digital Photos</div>
    <!-- Author -->
    <div class="author">
      

      Cong Yang, Zhenyu Yang, Yan Ke, Tao Chen, Marcin Grzegorzek, and <em>John See</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Image Processing</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/10006755" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://researchportal.hw.ac.uk/files/83582564/Doing_More_With_Moir_Pattern_Detection_in_Digital_Photos.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/cong-yang/MoireDet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/video/moiredet-demo.mp4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3232232"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3232232" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3232232" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Detecting moiré patterns in digital photographs is meaningful as it provides priors towards image quality evaluation and demoiréing tasks. In this paper, we present a simple yet efficient framework to extract moiré edge maps from images with moiré patterns. The framework includes a strategy for training triplet (natural image, moiré layer, and their synthetic mixture) generation, and a Moiré Pattern Detection Neural Network (MoireDet) for moiré edge map estimation. This strategy ensures consistent pixel-level alignments during training, accommodating characteristics of a diverse set of camera-captured screen images and real-world moiré patterns from natural images. The design of three encoders in MoireDet exploits both high-level contextual and low-level structural features of various moiré patterns. Through comprehensive experiments, we demonstrate the advantages of MoireDet: better identification precision of moiré images on two datasets, and a marked improvement over state-of-the-art demoiréing methods.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="slice-wtbpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/slice-wtb.png"><div id="slice-wtbpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('slice-wtbpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="slice-wtbpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("slice-wtbpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('slice-wtbpng');
      var modalImg = document.getElementById("slice-wtbpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="gohar2023slice" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Slice-Aided Defect Detection in Ultra High-Resolution Wind Turbine Blade Images</div>
    <!-- Author -->
    <div class="author">
      

      Imad Gohar, Abderrahim Halimi, <em>John See</em>, Weng Kean Yew, and Cong Yang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Machines</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.mdpi.com/2075-1702/11/10/953" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://www.mdpi.com/2075-1702/11/10/953/pdf?version=1697117170" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/imadgohar/DTU-annotations" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.3390/machines11100953"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.3390/machines11100953" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.3390/machines11100953" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The processing of aerial images taken by drones is a challenging task due to their high resolution and the presence of small objects. The scale of the objects varies diversely depending on the position of the drone, which can result in loss of information or increased difficulty in detecting small objects. To address this issue, images are either randomly cropped or divided into small patches before training and inference. This paper proposes a defect detection framework that harnesses the advantages of slice-aided inference for small and medium-size damage on the surface of wind turbine blades. This framework enables the comparison of different slicing strategies, including a conventional patch division strategy and a more recent slice-aided hyper-inference, on several state-of-the-art deep neural network baselines for the detection of surface defects in wind turbine blade images. Our experiments provide extensive empirical results, highlighting the benefits of using the slice-aided strategy and the significant improvements made by these networks on an ultra high-resolution drone image dataset.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="fatigueviewpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/fatigueview.png"><div id="fatigueviewpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('fatigueviewpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="fatigueviewpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("fatigueviewpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('fatigueviewpng');
      var modalImg = document.getElementById("fatigueviewpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="yang2022fatigueview" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection</div>
    <!-- Author -->
    <div class="author">
      

      Cong Yang, Zhenyu Yang, Weiyu Li, and <em>John See</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/9931532" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://pure.hw.ac.uk/ws/files/67344144/FatigueView_A_Multi_Camera_Video_Dataset_for_Vision_Based_Drowsiness_Detection.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/FatigueView/fatigueview" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://fatigueview.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TITS.2022.3216017"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1109/TITS.2022.3216017" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TITS.2022.3216017" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="skeletonGTpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/skeletonGT.png"><div id="skeletonGTpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('skeletonGTpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="skeletonGTpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("skeletonGTpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('skeletonGTpng');
      var modalImg = document.getElementById("skeletonGTpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="yang2023skeleton" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks</div>
    <!-- Author -->
    <div class="author">
      

      Cong Yang, Bipin Indurkhya, <em>John See</em>, Bo Gao, Yan Ke, Zeyd Boukhers, Zhenyu Yang, and Marcin Grzegorzek</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>International Journal of Computer Vision</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/article/10.1007/s11263-023-01926-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="http://arxiv.org/abs/2310.06437" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      <a href="https://researchportal.hw.ac.uk/files/104065760/s11263-023-01926-3.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/cong-yang/skeview" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1007/s11263-023-01926-3"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1007/s11263-023-01926-3" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1007/s11263-023-01926-3" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="ernetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ernet.png"><div id="ernetpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('ernetpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="ernetpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("ernetpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('ernetpng');
      var modalImg = document.getElementById("ernetpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="lim2023ernet" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">ERNet: An Efficient and Reliable Human-Object Interaction Detection Network</div>
    <!-- Author -->
    <div class="author">
      

      JunYi Lim, Vishnu Monn Baskaran, Joanne Mun-Yee Lim, KokSheik Wong, <em>John See</em>, and Massimo Tistarelli</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Transactions on Image Processing</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3231528"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3231528" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3231528" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" rel="external nofollow noopener" target="_blank">https://github.com/Monash-CyPhi-AI-Research-Lab/ernet</a>.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="megc2023png" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/megc2023.png"><div id="megc2023png-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('megc2023png-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="megc2023png-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("megc2023png-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('megc2023png');
      var modalImg = document.getElementById("megc2023png-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="davison2023megc2023" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">MEGC2023: ACM Multimedia 2023 ME Grand Challenge</div>
    <!-- Author -->
    <div class="author">
      

      Adrian K Davison, Jingting Li, Moi Hoon Yap, <em>John See</em>, Wen-Huang Cheng, Xiaobai Li, Xiaopeng Hong, and Su-Jing Wang</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>In Proceedings of the 31st ACM International Conference on Multimedia</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612833" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://megc2023.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3581783.3612833"></span>
      <span class="__dimensions_badge_embed__" data-doi="10.1145/3581783.3612833" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span>
      <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3581783.3612833" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" rel="external nofollow noopener" target="_blank"></a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. Unfortunately, the small sample problem severely limits the automation of ME analysis. Furthermore, due to the weak and transient nature of MEs, it is difficult for models to distinguish it from other types of facial actions. Therefore, ME in long videos is a challenging task, and the current performance cannot meet the practical application requirements. Addressing these issues, this challenge focuses on ME and the macro-expression (MaE) spotting task. This year, in order to evaluate algorithms’ performance more fairly, based on CAS(ME)2, SAMM Long Videos, SMIC-E-long, CAS(ME)3 and 4DME, we build an unseen cross-cultural long-video test set. All participating algorithms are required to run on this test set and submit their results on a leaderboard with a baseline result.</p>
    </div>
  </div>
</div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%6A.%73%65%65@%68%77.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-3005-4109" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=uH74dcgAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.semanticscholar.org/author/143937986" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a>
            <a href="https://publons.com/a/C-8633-2013/" title="Publons" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons"></i></a>
            <a href="https://www.scopus.com/authid/detail.uri?authorId=56227831000" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a>
            <a href="https://github.com/jstulips" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/john-see" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/johnseesuyang" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            <a href="https://dblp.org/pid/36/4809.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 John  See. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: February 28, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>
  <script async src="https://cdn.plu.mx/widget-popup.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
